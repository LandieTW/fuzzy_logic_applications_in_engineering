{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e3a0e19b-10b4-453d-a953-9ad0782c1992",
   "metadata": {},
   "source": [
    "Fuzzy modeling has been employed to generate a rule-base to model a syntactic foam from only limited input-output data obtained through various compreesive tests.\n",
    "Owing to their light weight and high compressive strength, foams have been incorporated into the design of many engineering systems.\n",
    "\n",
    "This is especially true in the aircraft industry where these characteristics are extremely important. For instance, aluminum sysntectic foam is theorized to have a shear strength that is 3 times greater than existing aluminum. Syntactic foams are composite materials formed by mechanically combining manufactured material bubbles or microspheres with resin. They are referred to as syntactic because the microspheres are arranged together, unlike blown foams that are created by injecting gas into a liquid slurry causing the bubbles to solidify and producing foam. \n",
    "\n",
    "As is often the case with newly developed materials, the cost of preparing the material is high; thus, only limited information is available on the material\n",
    "\n",
    "A newly developed syntactic foam has been selected as an encasing material because its light weight, high compressive strength, isotropic behavio, low porosity, and because the material is involved in preparing the syntactic foam, only a limited amount has been made.\n",
    "\n",
    "Fortunately, in addition to the two pieces of syntactic foam specimens prepared for use as the encasing material, four specimens were prepared for conducting triaxial compression tests. \n",
    "The four specimens each had a height of 2.800 +/- 0.001 inches and a diameter of 1.400 +/- 0.001 inches.\n",
    "The triaxial compression test is capable of appluing two different compressive stresses to the sample, a radial (minor principal) and a longitudinal (major principal) stress. In each test performed, the compressive pressure (stress) was gradually increased causing the sample to deform.\n",
    "\n",
    "The first test applied a continuous equivalent major and minor principal compressive stress to the specimen (hydrostatic compression) and the yield stress was observed at about 10_000 psi.\n",
    "In the second test, the major and minor principal stresses were gradually increased to a value of 3_750 psi; the minor principal stress was then held constant at this value, whereas the major principal stress was continuously increased.\n",
    "To probe a good portion of stress space, the same procedure was followed for the third and fourth tests; however, the minor principal stress was held constant at values of 6_500 and 9_000 psi, respectively, for these.\n",
    "\n",
    "In each of the previous tests, the experimentalist found that maintaining the minor principal stress constant was difficult and the minor principal stress was noted to fluctuate by as much as +/- 200 psi. The experimentalist also noted that at about the yielding stress the syntactic foam began to harden and exchibit nonlinear behavior.\n",
    "\n",
    "A portion of the nonlinear data collected from the triaxial compression tests:\n",
    "\n",
    "Training set\n",
    "Major       Minor       SigmaL\n",
    "(x1 psi)    (x2 psi)    (inch)\n",
    "12250       3750        3.92176E-2\n",
    "11500       6500        2.90297E-2\n",
    "11250       9000        2.51901E-2\n",
    "11000       11000       1.63300E-2\n",
    "11960       9000        2.50463E-2\n",
    "12140       6510        3.22360E-2\n",
    "13000       3750        4.37127E-2\n",
    "13800       3750        4.91016E-2\n",
    "12950       6500        3.60650E-2\n",
    "12600       9000        2.80437E-2\n",
    "11170       11110       1.65160E-2\n",
    "12930       13000       2.02390E-2\n",
    "13000       12500       2.36600E-2\n",
    "11130       9000        2.19333E-2\n",
    "11250       6500        2.77190E-2\n",
    "12000       3750        3.88243E-2\n",
    "12900       3750        4.28953E-2\n",
    "11990       6500        3.15040E-2\n",
    "12010       9000        2.58113E-2\n",
    "12000       12000       1.95610E-2\n",
    "14490       14450       2.41490E-2\n",
    "12900       9000        2.97153E-2\n",
    "13220       6500        3.81633E-2\n",
    "14000       3750        5.05537E-2\n",
    "\n",
    "Testing set\n",
    "Major       Minor       SigmaL\n",
    "(x1 psi)    (x2 psi)    (inch)\n",
    "12911.1     12927       2.0273E-2\n",
    "11092.3     10966.4     1.59737E-2\n",
    "13545.8     14487.1     2.40827E-2\n",
    "13012.1     12963.8     2.02157E-2\n",
    "12150       3750        3.8150E-2\n",
    "12904       3744.8      4.28953E-2\n",
    "14000       3770.4      5.05537E-2\n",
    "11406       6520.3      2.83967E-2\n",
    "12100       6535.5      3.2120E-2\n",
    "13109       6525.3      3.69773E-2\n",
    "11017.6     9000.7      2.11967E-2\n",
    "12105.9     8975.1      2.57443E-2\n",
    "12700       900.0       2.8504E-2\n",
    "\n",
    "Using the information provided by the experimentalist and the portion of the input-output data, develop a fuzzy model to obtain a general rule-base for governing the system, also to fine-tune the rule-base and the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16967f-7a9e-4012-8bef-c8f870f1e39a",
   "metadata": {},
   "source": [
    "BATCH LEAST SQUARES ALGORITHM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73802308-83e9-4b3a-b570-d0ba96e5266b",
   "metadata": {},
   "source": [
    "BLS ALGO\n",
    "\n",
    "The algorithm constructs a fuzzy model from numerical data, which can then be used to predict outputs given any input. Thus, the data set Z can be thought of as a training set used to model the system. When using the BLS algorithm to develop a fuzzy model it is helpful to have knowledge about the begavior of the data set to form a rule-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c13257-1277-43da-a876-fa12521dcf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0178acc4-61e8-4682-9962-27da7adfae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_train = np.array([\n",
    "    12250, 11500, 11250, 11000, 11960, 12140, 13000, 13800, 12950, 12600,\n",
    "    11170, 12930, 13000, 11130, 11250, 12000, 12900, 11990, 12010, 12000,\n",
    "    14490, 12900, 13220, 14000\n",
    "])\n",
    "\n",
    "minor_train = np.array([\n",
    "    3750, 6500, 9000, 11000, 9000, 6510, 3750, 3750, 6500, 9000,\n",
    "    11110, 13000, 12500, 9000, 6500, 3750, 3750, 6500, 9000, 12000,\n",
    "    14450, 9000, 6500, 3750\n",
    "])\n",
    "\n",
    "sigma_train = np.array([\n",
    "    3.92176e-2, 2.90297e-2, 2.51901e-2, 1.63300e-2, 2.50463e-2, 3.22360e-2,\n",
    "    4.37127e-2, 4.91016e-2, 3.60650e-2, 2.80437e-2, 1.65160e-2, 2.02390e-2,\n",
    "    2.36600e-2, 2.19333e-2, 2.77190e-2, 3.88243e-2, 4.28953e-2, 3.15040e-2,\n",
    "    2.58113e-2, 1.95610e-2, 2.41490e-2, 2.97153e-2, 3.81633e-2, 5.05537e-2\n",
    "])\n",
    "\n",
    "major_test = np.array([\n",
    "    12911.1, 11092.3, 13545.8, 13012.1, 12150, 12904, 14000,\n",
    "    11406, 12100, 13109, 11017.6, 12105.9, 12700\n",
    "])\n",
    "\n",
    "minor_test = np.array([\n",
    "    12927, 10966.4, 14487.1, 12963.8, 3750, 3744.8, 3770.4,\n",
    "    6520.3, 6535.5, 6525.3, 9000.7, 8975.1, 900.0\n",
    "])\n",
    "\n",
    "sigma_test = np.array([\n",
    "    2.0273e-2, 1.59737e-2, 2.40827e-2, 2.02157e-2, 3.8150e-2,\n",
    "    4.28953e-2, 5.05537e-2, 2.83967e-2, 3.2120e-2, 3.69773e-2,\n",
    "    2.11967e-2, 2.57443e-2, 2.8504e-2\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31f985d-3576-4899-bfe5-ea9d9edc29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.column_stack([major_train, minor_train])\n",
    "y_train = sigma_train\n",
    "\n",
    "X_test = np.column_stack([major_test, minor_test])\n",
    "y_test = sigma_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025dfc08-4b99-4b02-8750-8e54ecc6fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_bls_algo(\n",
    "    X_train: np.column_stack,\n",
    "    y_train: np.array,\n",
    "    X_test: np.column_stack,\n",
    "    y_test: np.array,\n",
    "    n_sets: int = 3\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Calculates fuzzy model with BLS algorithm\n",
    "\n",
    "    Args:\n",
    "        X_train: Train input\n",
    "        y_train: Train output\n",
    "        X_test: Test input\n",
    "        y_test: Test output\n",
    "\n",
    "    Returns:\n",
    "        b: consequent parameter\n",
    "        train_pred: train prediction\n",
    "        test_pred: test prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalizing data\n",
    "    x1_train_norm = (\n",
    "        (X_train[:, 0] - np.mean(X_train[:, 0])) / np.std(X_train[:, 0])\n",
    "    )\n",
    "    x2_train_norm = (\n",
    "        (X_train[:, 1] - np.mean(X_train[:, 1])) / np.std(X_train[:, 1])\n",
    "    )\n",
    "    x1_test_norm = (\n",
    "        (X_test[:, 0] - np.mean(X_test[:, 0])) / np.std(X_test[:, 0])\n",
    "    )\n",
    "    x2_test_norm = (\n",
    "        (X_test[:, 1] - np.mean(X_test[:, 1])) / np.std(X_test[:, 1])\n",
    "    )\n",
    "\n",
    "    # number of rules\n",
    "    R = n_sets ** 2\n",
    "\n",
    "    # defining the centers\n",
    "    c1 = np.linspace(np.min(x1_train_norm), np.max(x1_train_norm), n_sets)\n",
    "    c2 = np.linspace(np.min(x2_train_norm), np.max(x2_train_norm), n_sets)\n",
    "\n",
    "    # Defining widths\n",
    "    sigma1 = (np.max(x1_train_norm) - np.min(x1_train_norm)) / n_sets\n",
    "    sigma2 = (np.max(x2_train_norm) - np.min(x2_train_norm)) / n_sets\n",
    "\n",
    "    # Matrix\n",
    "    def matrix(\n",
    "        x1_norm: np.column_stack,\n",
    "        x2_norm: np.column_stack\n",
    "    ) -> np.array:\n",
    "        \"\"\"\n",
    "        Generate the regression matrix for fuzzy model\n",
    "\n",
    "        Args:\n",
    "            x1_norm:\n",
    "            x2_norm:\n",
    "\n",
    "        Returns:\n",
    "            Phi matrix\n",
    "        \"\"\"\n",
    "        N = len(x1_norm)\n",
    "        Phi = np.zeros((N, R))\n",
    "\n",
    "        for i in range(N):\n",
    "            w = list()\n",
    "            for j in range(n_sets):\n",
    "                for k in range(n_sets):\n",
    "\n",
    "                    # membership functions\n",
    "                    mu_1 = np.exp(-.5 * (x1_norm[i] - c1[j]) / sigma1)**2\n",
    "                    mu_2 = np.exp(-.5 * (x2_norm[i] - c2[j]) / sigma2)**2\n",
    "\n",
    "                    # rule weight\n",
    "                    w.append(mu_1 * mu_2)\n",
    "                    \n",
    "            # sum the weights\n",
    "            w_sum = sum(w)\n",
    "            \n",
    "            if w_sum == 0:\n",
    "                w_sum = 1e-10   # avoid zero division\n",
    "                \n",
    "            Phi[i, :] = [w_i / w_sum for w_i in w]\n",
    "\n",
    "        return Phi\n",
    "\n",
    "    Phi_train = matrix(x1_train_norm, x2_train_norm)\n",
    "\n",
    "    # bls = (ΦᵀΦ)⁻¹ Φᵀ y\n",
    "    bls = np.linalg.pinv(Phi_train.T @ Phi_train) @ Phi_train.T @ y_train\n",
    "\n",
    "    y_train_pred = Phi_train @ bls\n",
    "\n",
    "    Phi_test = matrix(x1_test_norm, x2_test_norm)\n",
    "    y_test_pred = Phi_test @ bls\n",
    "\n",
    "    return bls, y_train_pred, y_test_pred, c1, c2, sigma1, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53d8eca-4a1b-4a77-bbac-530fba8b910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bls, y_train_pred, y_test_pred, c1, c2, sigma1, sigma2 = fuzzy_bls_algo(\n",
    "    X_train, y_train, X_test, y_test, n_sets=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afd1660-8902-4a37-b9e3-e83ecba0014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors\n",
    "\n",
    "rmse_train = np.sqrt(np.mean((y_train - y_train_pred)**2))\n",
    "rmse_test = np.sqrt(np.mean((y_test - y_test_pred)**2))\n",
    "\n",
    "mae_train = np.mean(np.abs(y_train - y_train_pred))\n",
    "mae_test = np.mean(np.abs(y_test - y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a66c9f-dbed-4a26-a3ae-f35014665572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLS Algo\n",
      "Rule 1: b = 0.000080\n",
      "Rule 2: b = 0.000080\n",
      "Rule 3: b = 0.000080\n",
      "Rule 4: b = 0.001601\n",
      "Rule 5: b = 0.001601\n",
      "Rule 6: b = 0.001601\n",
      "Rule 7: b = 0.032155\n",
      "Rule 8: b = 0.032155\n",
      "Rule 9: b = 0.032155\n",
      "Training Errors\n",
      "RMSE: 0.009560\n",
      "MAE: 0.007994\n",
      "Testing Errors\n",
      "RMSE: 0.009776\n",
      "MAE: 0.008324\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "print(\"BLS Algo\")\n",
    "for i, bls in enumerate(bls):\n",
    "    print(f\"Rule {i+1}: b = {bls:.6f}\") \n",
    "\n",
    "print(\"Training Errors\")\n",
    "print(f\"RMSE: {rmse_train:.6f}\")\n",
    "print(f\"MAE: {mae_train:.6f}\")\n",
    "\n",
    "print(\"Testing Errors\")\n",
    "print(f\"RMSE: {rmse_test:.6f}\")\n",
    "print(f\"MAE: {mae_test:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a27f8d-b539-489a-9800-a84be4795222",
   "metadata": {},
   "source": [
    "RECURSIVE LEAST SQUARES ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91add843-b3ef-4558-8796-f414c9e005ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e42d39-a0ee-4fcc-a344-1859c1c1b8c1",
   "metadata": {},
   "source": [
    "GRADIENT METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88158f2c-a599-43d3-a738-e465183283d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c21d2de-3996-451e-a804-72ca15ebc0e5",
   "metadata": {},
   "source": [
    "CLUSTERING METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63ea67-c122-44d6-8477-251cfeddb65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f2315db-372a-421c-858a-718bd4c168e6",
   "metadata": {},
   "source": [
    "LEARNING FROM EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdad5f8-2a38-40a9-9b3f-cddd600e9da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dccfeed3-6849-4b55-9e89-5b67da0f86d7",
   "metadata": {},
   "source": [
    "MODIFIED LEARNING FROM EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827a192-b959-4053-af00-7edec9fb2b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
